---
title: "Boston Energy Forecast"
author: "Sameer, Lalit, Lipsa"
date: "October 28, 2016"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
  word_document:
    keep_md: yes
    toc: yes
---

\pagebreak

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r ,message=FALSE,warning=FALSE,echo=FALSE}
setwd("D:/Fall 16/ADS/Git Repo Assignemnt 2- Team 7")
```

##1. Introduction
The report introduces to the dataset of electricity consumption of the Mildred School of Boston Area.The primary purpose was to demonstrate a forecast model to predict energy usage of Boston city which can play an important role in the current and future electricity generation. We tried to infer the  relationship from various factors that potentially impacts the energy consumption in the city.

###1.1 About the data
The data set that is take into consideration is derived from two sources: 
-one is the data collected in 2014 at Mildred School.
-Second is the hourly weather data pulled from https://www.wunderground.com using API. 

###1.1 Mildred school energy usage dataset
The initial dataset contains Boston energy usage captured at every 5 mins interval for a particular account "26908650026" and channel "MILDRED SCHOOL 1".

####1.1.1 Data description
The study data  was given in two flat files of 4MB each. We imported the datasets from our working directory as:
```{r ,message=FALSE,warning=FALSE}
setwd("D:/Fall 16/ADS/Git Repo Assignemnt 2- Team 7")
rd1 <-  read.csv("D:/Fall 16/ADS/Git Repo Assignemnt 2- Team 7/InputFiles/rawData1.csv",header = TRUE)
rd2 <- read.csv("D:/Fall 16/ADS/Git Repo Assignemnt 2- Team 7/InputFiles/rawData2.csv",header = TRUE)
```

####1.1.2 Description of rows and columns
To have an insight of the data, we took a closer look at the 4 variables `Account`, `Date`, `Channel` and `Units` recorded at every 5 min time interval.


Variable      | Type          | Range        | Description
------------- | ------------- |------------- |--------------------------------------------------------------
`Account`     | Numeric       | 26908650026  | Account number belonging to Mildred School
`Date`        | Date          | Jan-Dec'14   | Complete 2014 year data
`Channel`     | Factor        | 7 Channels   | All Channels linked to the account, including                                                 Mildred School
`Units`       | Factor        | 2 Units      | Units in which usage is recorded. kWh and kVARh 

###1.2 Dataset from wunderground.com
The second dataset can be fetched from the website containing weather data wunderground.com with the help of the "weatherData" package . The package gives all the weather related information taking location and date range as input.

####1.2.1 Data description
We have extracted the  `Temperature`, `Dew_PointF`, `Humidity`, `Sea_Level_PressureIn`, `VisibilityMPH`, `Wind_Direction`, `Wind_SpeedMPH`, `Conditions` and `WindDirDegrees` data for Boston (KBOS) for 2014 Year.

####1.2.2 Description of rows and columns
The weather data is closely understood by looking a columns and their characteristics.

Variable               | Type      | Unit      | Range    | Description
---------------------- | --------- |---------- | -------- |--------------------------------------------------------------
`TemperatureF`         | Numeric   | Fahrenheit|  0 - 100 | Temperature in atmosphere

`Dew_PointF`           | Numeric   | Fahrenheit|-20 -  80 | Temperature to which the air needs                                                             to be cooled to make the air                                                                  saturated

`Humidity`             | Numeric   | Percent(%)| 10 - 100 | The water content of the air
`Sea_Level_PressureIn` | Numeric   | Pascal    | 28 -  32 | Atmosphere exerts a pressure on                                                               the surface of the earth at sea                                                               level. 
`VisibilityMPH`        | Numeric   | Miles/Hour|  0 -  10 | Measure of the distance at which                                                              an object or light can be clearly                                                             discerned.
`Wind_SpeedMPH`        | Numeric   | Miles/Hour|  0 -  50 | Wind speed in MPH
`Conditions`           | Numeric   | Condition |    --    | state of atmosphere in temp,wind.
`WindDirDegrees`       | Numeric   | Degrees   |  0 - 360 | In which direct wind is moving

##2. Goal

The primary aim was to build and validate a forecasting model that will predict the energy usage in future years by computing two datasets. We  applied the multi-linear regression to model Power usage as a function of multiple variables. To supply this model we need to first do data wrangling, then Algorithm implementation and finally forecast.

##3. Process Flow
Data wrangling -> Feature selection -> Algorithm Implementation -> Forecasting Model -> Prediction

###3.1 Installing Packages and libraries
To process the data we will import required packages and libraries

####3.1.1 dplyr package
We will install and load the dplyr package that contains additional functions for data manipulation using data frames. It allows us to order rows, select rows, select variables, modify variables and summarize variables. We will be using functions like distinct, filter, group by, n_distinct from this package.
```{r, message=FALSE,warning=FALSE}
library(dplyr)
```

####3.1.2 tidyr package
It's designed specifically for data tidying (not general reshaping or aggregating) and works well with 'dplyr' data pipelines.
```{r, message=FALSE,warning=FALSE}
library(tidyr)
```


####3.1.3 lubridate package
Functions to work with date-times and time-spans: fast and user friendly parsing of date-time data, extraction and updating of components of a date-time (years, months, days, hours, minutes, and seconds), algebraic manipulation on date-time and time-span objects. The 'lubridate' package has a consistent and memorable syntax that makes working with dates easy and fun.
```{r, message=FALSE,warning=FALSE}
library(lubridate)
```

####3.1.4 weatherData package
Functions that help in fetching weather data from websites. Given a location and a date range, these functions help fetch weather data (temperature, pressure etc.) for any weather related analysis. 
```{r, message=FALSE,warning=FALSE}
library(weatherData)
```
install.packages("weatherData")

###3.2 Data wrangling and prepration
Data munging or data wrangling is loosely the process of manually converting or mapping data from one "raw" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools.

####3.2.1 Primary dataset filteration
As the dataset contains channels 7 different channels and we are interested in only kwh units for Mildred School
we will do initial dataset filtering and merging.

* filter(): Applies linear filtering to a uni-variate time series or to each series separately of a multivariate time series.
* rbind(): Take a sequence of vector, matrix or data-frame arguments and combine by columns or rows, respectively. 
* 
```{r ,message=FALSE,warning=FALSE}
# Filtering
rd1 <-  filter(rd1, Units == "kWh",Channel  == "MILDRED SCHOOL 1")
rd2 <-  filter(rd2, Units == "kWh",Channel  == "MILDRED SCHOOL 1")

# Merging
data <- rbind(rd1,rd2) 
```
####3.2.2 Datatype conformity
As we read the data from dataset, r intuitively understands datatypes. We need to check structure of our data and then change the columns structure according to our requirements.

`str()` function gives us the snapshot of datatypes of all the columns
```{r ,message=FALSE,warning=FALSE}
str(data[1:4])
```

We noticed that the Date , Channel and Units are Factor variables with levels .We converted the datatype of these columns in to as.Date() and as.character() function which were available in the base r library.

```{r ,message=FALSE,warning=FALSE}
data$Date <- as.Date(data$Date,"%m/%d/%Y")
data$Channel <- as.character(data$Channel)
data$Units <- as.character(data$Units)

# Re-check structure of data
str(data[1:4])
```

As we can see that the data is widely spread and for any analysis to be done we need to convert the data into long format. We will first find the positions of the columns and also save the columnnames into array.

* grep(): Search for matches to argument pattern within each element of a character vector
```{r,message=FALSE,warning=FALSE}
column_Pos <- grep("X",colnames(data))
```

This wills store the names of the columns for the observations
```{r,message=FALSE,warning=FALSE}
columnnamesval <- grep("X",colnames(data),value = TRUE)
```
###3.3 Data Gathering
We gathered the valid data to make it fit for analysis.

####3.3.1 Gathering the data  in long format.
Though we found the data was spread with the electricity consumption value spread out across columns.We tried to reformat the data such that these common attributes are together into a single value

* gather() :takes the column value and collapses them in to key-value pairs.
* head() : Returns the first parts of a vector, matrix, table, data frame or function.

```{r,message=FALSE,warning=FALSE}
Data.long <- gather(data,key,values,column_Pos)
head(Data.long)
```

####3.3.2 Derive data
Now we have the data in the long format we can apply some aggregations on the data for each hour which will comprise of 12 observations for each hour. We will create 24 new columns which will be sum of 12 observations of each hour using dplyr summarize and group by function.

Columns to be derived:

Column Name   | Description
------------- | ------------------------------------------------------------
`kWh`         | Sum of 12 observations (5 min intervals rolled up to hourly)
`month`       | 1-12 => Jan-Dec - Derived from dates
`day`         | 1-31 - Derived from dates
`year`        | Derived from dates
`hour`        | 0-23 - Derived for each record corresponding to the hour of observation
`Day of Week` | 0-6 -Sun-Sat - Derived from dates
`Weekday`     | 1- Yes 0- No - Derived from dates
`Peakhour`    | 7AM-7PM - 1 ; 7PM-7AM - 0

Below is a user defined function to find the Weekday

```{r,message=FALSE,warning=FALSE}
checkWeekday <- function(date){
  a = if((wday(date) == 6 || wday(date) == 7)) 0 else 1
  return(a)
}
```

We Will use dplyr chaining function to group the data first and summarize the columns for each hour using the 12 observations/each hour.

* group_by():  It breaks down a dataset into specified groups of rows. 
* summarise(): Summarise multiple values to a single value.
* mutate(): Mutate adds new variables and preserves existing; transmute drops existing variables.

```{r,message=FALSE,warning=FALSE}
aggData <- Data.long %>% group_by(Account,Date,Channel,Units) %>% 
  summarise('0' = sum(values[key %in% columnnamesval[1:12]]),
            '1' = sum(values[key %in% columnnamesval[13:24]]),
            '2' = sum(values[key %in% columnnamesval[25:36]]),
            '3' = sum(values[key %in% columnnamesval[37:48]]),
            '4'  = sum(values[key %in% columnnamesval[49:60]]),
            '5' = sum(values[key %in% columnnamesval[61:72]]),
            '6' = sum(values[key %in% columnnamesval[73:84]]),
            '7' = sum(values[key %in% columnnamesval[85:96]]),
            '8' = sum(values[key %in% columnnamesval[97:108]]),
            '9' = sum(values[key %in% columnnamesval[109:120]]),
            '10' = sum(values[key %in% columnnamesval[121:132]]),
            '11' = sum(values[key %in% columnnamesval[133:144]]),
            '12' = sum(values[key %in% columnnamesval[145:156]]),
            '13' = sum(values[key %in% columnnamesval[157:168]]),
            '14' = sum(values[key %in% columnnamesval[169:180]]),
            '15' = sum(values[key %in% columnnamesval[181:192]]),
            '16' = sum(values[key %in% columnnamesval[193:204]]),
            '17' = sum(values[key %in% columnnamesval[205:216]]),
            '18' = sum(values[key %in% columnnamesval[217:228]]),
            '19' = sum(values[key %in% columnnamesval[229:240]]),
            '20' = sum(values[key %in% columnnamesval[241:252]]),
            '21' = sum(values[key %in% columnnamesval[253:264]]),
            '22' = sum(values[key %in% columnnamesval[265:276]]),
            '23' = sum(values[key %in% columnnamesval[277:288]])) %>%
  mutate(month = lubridate::month(Date),day = day(Date),year = year(Date),'Day of Week' = wday(Date)) %>%
  mutate(weekday = sapply(Date, function(x) checkWeekday(x)))
```

Gathering the column again in long format so that data is in clean consistent format. This way we have the kWH value for every hour and for every day of the year 2014
```{r,message=FALSE,warning=FALSE}
aggData.long <- gather(aggData,hour,Kwh,5:28)
```

We will change the hour column into numeric
```{r,message=FALSE,warning=FALSE}
aggData.long$hour <- as.numeric(aggData.long$hour)
```

Now lets derive peak hour. Using the table definition we can define a function based on hours like below
```{r,message=FALSE,warning=FALSE}
cal_PeakHour <- function(hour){
  p = if(hour > 6 & hour < 20) 1 else 0
  return(p)
}
```
* sapply(): Each element of which is the result of applying FUN to the corresponding element of X.
```{r,message=FALSE,warning=FALSE}
aggData.long$PeakHour <- sapply(aggData.long$hour,function(x) cal_PeakHour(x))
```
Lets have a look at the structure of our dataset now
```{r,message=FALSE,warning=FALSE}
str(aggData.long)
```
The structure seems to align with our requirement now lets pull up the other dataset

####3.3.3 Data from weather API
We have used the WeatherData Package to pull all the weather related information from wunderground.com.The weatherData package takes a date range and Location as an input  .

We first calculated the minimum and max date for our observed dataset
```{r,message=FALSE,warning=FALSE}
mindate <- min(aggData.long$Date)
maxdate <- max(aggData.long$Date)
```

Converted the date range in to a desired format
```{r,message=FALSE,warning=FALSE,echo=False}
mindate <- as.Date(min(aggData.long$Date), "%m/%d/%Y")
maxdate <- as.Date(max(aggData.long$Date), "%m/%d/%Y")
```

We got the station code for Boston 
```{r,message=FALSE,warning=FALSE}
getStationCode("Boston")
```

The station_id for Boston is "KBOS"

* getWeatherForDate(): Getting data for a range of dates, it has certain parameters
* `station_id`: is a valid 3- or 4-letter Airport code or a valid Weather Station ID (example: "KBOS" for Boston).
* `start_date`: string representing a date in the past ("YYYY-MM-DD", all numeric)
* `end_date`  : If an interval is to be specified,end_date is a string representing a date in the past ("YYYY-MM-DD", all numeric) and greater than the start date
* `opt_detailed`:indicates if detailed records for the station are desired. (default FALSE). By default only one records per date is returned.
* `opt_custom_columns`:  to indicate if only a user-specified set of columns are to be returned. (default FALSE) If TRUE, then the desired columns must be specified via custom_columns 
* `custom_columns`: Vector of integers specified by the user to indicate which columns to fetch. The Date column is always returned as the first column.

Once we fetched the respective inputs for the WeatherData .We tried to extract the weather information with the applied inputs.

```{r,message=FALSE,warning=FALSE}
WeatherData <- getWeatherForDate("KBOS", start_date=mindate,
                                 end_date = maxdate,
                                 opt_detailed=T,opt_custom_columns=T,
                                 custom_columns=c(2:13))
```

##4.Data Analysis

###4.1 Insight on Weather Data

```{r,message=FALSE,warning=FALSE}
head(WeatherData)
```

We calculated the date and hour using the "Lubricate" package we have used.

```{r,message=FALSE,warning=FALSE}
WeatherData$date = date(WeatherData$Time)
WeatherData$hour = hour(WeatherData$Time)
```

After looking in to the information pulled by the WeatherData package ,we got a picture that data is spread on hourly interval. We tried to confirm with the following function.

```{r,message=FALSE,warning=FALSE}
table(WeatherData$date)
```

After looking at the tabular values, we deduced that although most of the days had 24 observations, some of them have more than 24 .

The details revealed that in some instances observations were taken more than once for each hour,as illustrated in the following case :
```{r,message=FALSE,warning=FALSE}
View(WeatherData[which(WeatherData$date == "2014-06-05"),])
```

Detail Observation :

* we  got -999999 value in columns TempratureF, DewPointF, Sea_Level_PressureIn, Visibility MPH
* We converted the data to the respective data types
* WindSpeed "Calm" which mean 0: Converting to character as it is in factor

```{r,message=FALSE,warning=FALSE}
WeatherData$date <-  as.Date(WeatherData$date,"%m/%d/%Y")
WeatherData$TemperatureF <- as.numeric(WeatherData$TemperatureF)
WeatherData$Dew_PointF <- as.numeric(WeatherData$Dew_PointF)
WeatherData$Sea_Level_PressureIn <- as.numeric(WeatherData$Sea_Level_PressureIn)
WeatherData$VisibilityMPH <- as.numeric(WeatherData$VisibilityMPH)
WeatherData$WindDirDegrees <- as.numeric(WeatherData$WindDirDegrees)
WeatherData$Humidity <- as.numeric(WeatherData$Humidity)

WeatherData$Wind_SpeedMPH[WeatherData$Wind_SpeedMPH == "Calm"] <- 0
WeatherData$Wind_SpeedMPH <- as.numeric(WeatherData$Wind_SpeedMPH)
```

We need our data to fall in normal range to remove outliers

###4.2 Handling Outliers

* We used the approach of substituting the previous or the next value of the observation.
  For example, if the record 8999 has Temperature as -9999 we  used the record of 8998 so that   this is still acceptable.

* We tried to handle the outliers with the following function

```{r,message=FALSE,warning=FALSE}
remove_out <- function(param,index,min_v,max_v)
{
  val = NULL
  val = param[index]
  
  if(val < min_v | val > max_v | is.na(val)){
    if(index-1 >= 1){
      val = param[index-1]
    } else if (index-1 <= 0){
      val = param[index+1]
    } 
    return(val)
  } else{
    print("Nothing changed")
    return(val)  #Normal Value return
  }
}

```

With the above function  removed the outliers for Temperature. We found out the records where Temperature is falling out of the range defined in the table.


Temperature
 
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$TemperatureF < 0 | WeatherData$TemperatureF > 100 | is.na(WeatherData$Dew_PointF))
print(index)
```


We had an insight in to the data records
WeatherData[8206,]


We found that it was indeed an outlier,could be a machine input error. We tried to remove this implementing the function and checked the record again after the function 
```{r,message=FALSE,warning=FALSE}
for (i in index){
WeatherData$TemperatureF[i] = remove_out(WeatherData$TemperatureF,i,0,100)
}
WeatherData[8206,]
```

We were successful in getting in to shape.We implemented the same thing for the other features:

Dew Point
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$Dew_PointF < -20 | WeatherData$Dew_PointF > 80 | is.na(WeatherData$Dew_PointF))
for (i in index){
  WeatherData$Dew_PointF[i] = remove_out(WeatherData$Dew_PointF,i,-20,80)
}
```

Humidity
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$Humidity < 10 | WeatherData$Humidity > 100 | is.na(WeatherData$Humidity))
for (i in index){
  WeatherData$Humidity[i] = remove_out(WeatherData$Humidity,i,10,100)
}
```

Wind_SpeedMPH
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$Wind_SpeedMPH < 0 | WeatherData$Wind_SpeedMPH > 50 | is.na(WeatherData$Wind_SpeedMPH))
for (i in index){
  WeatherData$Wind_SpeedMPH[i] = remove_out(WeatherData$Wind_SpeedMPH,i,0,50)
}
```

Sea_Level_Pressure
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$Sea_Level_PressureIn < 28 | WeatherData$Sea_Level_PressureIn > 32 | is.na(WeatherData$Sea_Level_PressureIn))
for (i in index){
  WeatherData$Sea_Level_PressureIn[i] = remove_out(WeatherData$Sea_Level_PressureIn,i,28,32)
}
```

VisibilityMPH
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$VisibilityMPH < 0 | WeatherData$VisibilityMPH > 10 | is.na(WeatherData$VisibilityMPH))
for (i in index){
  WeatherData$VisibilityMPH[i] = remove_out(WeatherData$VisibilityMPH,i,0,10)
}
```

WindDirDegree
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$WindDirDegrees < 0 | WeatherData$WindDirDegrees > 360 | is.na(WeatherData$WindDirDegrees))
for (i in index){
  WeatherData$WindDirDegrees[i] = remove_out(WeatherData$WindDirDegrees,i,0,360)
}
```

The data was clean and consistent. We aggregated the dataset as done in part 1, so that we get records for each hour and we can take average values for numeric values and frequency count for character values.
We followed the below steps:
 1) Remove non essential features like Time, Gust_speedMH,P,E
 2) Group the data by Date and hour 
 3) summarise base on mean and frequency count

```{r,message=FALSE,warning=FALSE}
WeatherData.Agg <- WeatherData %>% 
  select(-c(Time,Gust_SpeedMPH,
            PrecipitationIn,Events)) %>%
  group_by(date,hour) %>% 
  summarise(TemperatureF = mean(TemperatureF),
                                    Dew_PointF = mean(Dew_PointF),
                                    Humidity = mean(Humidity),
                                    Sea_Level_PressureIn = mean(Sea_Level_PressureIn),
                                    VisibilityMPH = mean (VisibilityMPH),
                                    Wind_SpeedMPH = mean(Wind_SpeedMPH),
                                    WindDirDegrees = mean(WindDirDegrees),
            Conditions = names(table(Conditions))[which.max(table(Conditions))],
            Wind_Direction = names(table(Wind_Direction))[which.max(table(Wind_Direction))])

head(WeatherData.Agg)
```

Once We had both the dataset wit  in the desired format
We merged the data with part 1 of the energy usage data by Date and hour

###4.3 Final Output Data
```{r,message=FALSE,warning=FALSE}
mergeData <- merge(aggData.long,WeatherData.Agg,by.x = c("Date","hour"),by.y = c("date","hour"))
head(mergeData)

```

Arranging the data by Date and hour
```{r,message=FALSE,warning=FALSE}
mergeData<- arrange(mergeData,Date,hour)
head(mergeData)
```

We will save this dataset for future use
```{r,message=FALSE,warning=FALSE}
write.csv(mergeData,"Part1/Output/MergedData.csv")
```

##5 Multiple-Linear Regression

Multiple linear regression is the most common form of linear regression analysis.  As a predictive analysis, the multiple linear regression is used to explain the relationship between one continuous dependent variable from two or more independent variables.  The independent variables can be continuous or categorical (dummy coded as appropriate).

###5.1 Applying Multiple-Linear Regression to our dataset

####5.1.1 Dataset import and structure reset
We will import and check the structure of our cleansed dataset to be used to build our model

```{r, message=FALSE, warning=FALSE}
setwd("setwd("D:/Fall 16/ADS/Git Repo Assignemnt 2- Team 7/Part1/Input)
mergeData <-  read.csv("MergedData.csv",header = TRUE)
str(mergeData)
```
We would like to restructure some of the columns of the dataset and remove the unwanted columns, the datatypes of columns is understood intuitively by r, so we have to make sure the correct structure before proceeding for computation.

```{r, message=FALSE, warning=FALSE}
require(dplyr)
mergeData <- mergeData %>% select(-c(X,Date,Account,Channel,Units,year))

mergeData$hour <- as.numeric(mergeData$hour)
mergeData$month <- as.numeric(mergeData$month)
mergeData$Day.of.Week <-  as.numeric(mergeData$Day.of.Week)
mergeData$weekday <-  as.numeric(mergeData$weekday)
mergeData$PeakHour <- as.numeric(mergeData$PeakHour)
mergeData$day <- as.numeric(mergeData$day)
```
We remove the categorical columns because it will not be useful in regression model and keeping subset of the data frame which only contains numeric features.
```{r, message=FALSE, warning=FALSE}
merge.sel <- subset(mergeData,select = -c(Conditions,Wind_Direction))
# Re-Check the structure
str(mergeData)
```
The structure of the dataset is now ready for computation.

####5.1.2 Feature Transformation

There are multiple methods of feature creation and data transformation. Often, finding the right transformation of your data can reveal relationships that would be difficult see otherwise, and may also make it easier for your model to separate classes. In the simple case of creating linear models for regression, this can take the form of squaring terms, taking their logarithm, or other functional transformations that bring the fundamental problem back into a linear relationship through the transformation.

We will start by checking non linearity in the model. We will use the library psych

* psych: Adapted from the help page for pairs, pairs.panels shows a scatter plot of matrices (SPLOM), with bi variate scatter plots below the diagonal, histograms on the diagonal, and the Pearson correlation above the diagonal.
```{r, message=FALSE, warning=FALSE}
require(psych)
#pairs.panels(merge.sel,col="red")
```
Using this plot we can identify the non linearity. To take a closer look we plot month.

```{r, message=FALSE, warning=FALSE}
plot(density(merge.sel$Kwh))
```

As seen from the distribution of KwH, we will use a log scale to convert the feature, which is nothing but our dependent variable.

We plot again with transformed predictor i.e. KwH to check if the distribution still looks skewed as in last plot 

```{r, message=FALSE, warning=FALSE}
plot(density(log10(merge.sel$Kwh)))
```

The plot looks very much distributed to we will update the transformed predictor value in our dataset as well.

```{r, message=FALSE, warning=FALSE}
merge.sel$Kwh <- log10(merge.sel$Kwh)
```
Our next focus is to check other feature with respect to kwH and see the relationship. Lets check the histogram of Day and also the relationship with kwH
```{r, message=FALSE, warning=FALSE}
hist(merge.sel$hour, xlab = 'hour')
```

We plot this graph again using scatter.smooth() function.
* scatter.smooth():Plot and add a smooth curve computed by loses to a scatter plot.
```{r, message=FALSE, warning=FALSE}
scatter.smooth(x=merge.sel$Kwh,y=merge.sel$hour,xlab = 'KwH', ylab = 'hour')
```

In r we have other methods for feature transformation, we find lambda transformation to be useful to demonstrate with `Day Of Week` parameter.

* forecast library: Methods and tools for displaying and analyzing uni variate time series forecasts including exponential smoothing via state space models and automatic ARIMA modelling.

* BoxCox.lambda(): It is procedure to identify an appropriate exponent (lambda) to transform the data to improve its normality.

We use lambda transformation for `Day Of Week` and update the transformed value to our dataset.
```{r, message=FALSE, warning=FALSE}
library(forecast)
lambda <- BoxCox.lambda(merge.sel$Day.of.Week)
merge.sel$Day.of.Week <- BoxCox(merge.sel$Day.of.Week,lambda = lambda)
```

####5.1.3 Feature Selection
Selecting the right features in your data can mean the difference between mediocre performance with long training times and great performance with short training times, thus we do feature selection.


We will use leaps library for feature selection. leaps() performs an exhaustive search for the best subsets of the variables in x for predicting y in linear regression, using an efficient branch-and-bound algorithm. It is a compatibility wrapper for regsubsets does the same thing better.
```{r, message=FALSE, warning=FALSE}
library(leaps)
```

####5.1.3.1 Feature Selection Exhaustive Search

* regsubsets(): Model selection by exhaustive search, forward or backward stepwise, or sequential replacement
```{r, message=FALSE, warning=FALSE}
regsubsets.out <- regsubsets(Kwh ~ ., data = merge.sel, nvmax = 14,method = "exhaustive")
res <- summary(regsubsets.out)
```
The regsubsets()function has a built-in plot command which can display the selected variables for the "best" model with a given model selection statistic.  The top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic.  Examples using the R2 (unadjusted), adjusted R2, Mallow's Ck, and the BIC are shown as below.

```{r, message=FALSE, warning=FALSE}
plot(res$cp, main = "CP") 
```
```{r, message=FALSE, warning=FALSE}
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
```
```{r, message=FALSE, warning=FALSE}
plot(regsubsets.out, scale = "bic", main = "BIC")
```
```{r, message=FALSE, warning=FALSE}
plot(res$adjr2, main = "adj r^2")
```
```{r, message=FALSE, warning=FALSE}
coef(regsubsets.out,10)
res$adjr2[10]
```

####5.1.3.2 Feature Selection Forward  Search

```{r, message=FALSE, warning=FALSE}
regsubsets.out <- regsubsets(Kwh ~ ., data = merge.sel, nvmax = 14,method = "forward")
res <- summary(regsubsets.out)
```
Statistics:

```{r, message=FALSE, warning=FALSE}
plot(res$cp, main = "CP") 
```
```{r, message=FALSE, warning=FALSE}
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
```
```{r, message=FALSE, warning=FALSE}
plot(regsubsets.out, scale = "bic", main = "BIC")
```
```{r, message=FALSE, warning=FALSE}
plot(res$adjr2, main = "adj r^2")
```
```{r, message=FALSE, warning=FALSE}
coef(regsubsets.out,10)
res$adjr2[10]
```

####5.1.3.3 Feature Selection Backward Search

```{r, message=FALSE, warning=FALSE}
regsubsets.out <- regsubsets(Kwh ~ ., data = merge.sel, nvmax = 14,method = "backward")
res <- summary(regsubsets.out)
```
Statistics:

```{r, message=FALSE, warning=FALSE}
plot(res$cp, main = "CP") 
```
```{r, message=FALSE, warning=FALSE}
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
```
```{r, message=FALSE, warning=FALSE}
plot(regsubsets.out, scale = "bic", main = "BIC")
```
```{r, message=FALSE, warning=FALSE}
plot(res$adjr2, main = "adj r^2")
```
```{r, message=FALSE, warning=FALSE}
coef(regsubsets.out,10)
res$adjr2[10]
```

####5.1.3.4 Feature Selection Manually selecting features

```{r, message=FALSE, warning=FALSE}
regsubsets.step <- regsubsets(Kwh~hour + month + day + Day.of.Week + weekday + PeakHour +
                                TemperatureF + Dew_PointF + Humidity +
                                VisibilityMPH, data = merge.sel, nvmax =14 )
summary(regsubsets.step)
resstep <- summary(regsubsets.step)
```
Statistics:

```{r, message=FALSE, warning=FALSE}
plot(res$cp, main = "CP") 
```
```{r, message=FALSE, warning=FALSE}
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
```
```{r, message=FALSE, warning=FALSE}
plot(regsubsets.out, scale = "bic", main = "BIC")
```
```{r, message=FALSE, warning=FALSE}
plot(res$adjr2, main = "adj r^2")
```
```{r, message=FALSE, warning=FALSE}
coef(regsubsets.out,10)
res$adjr2[10]
```

####5.1.4 Develop a linear model

Now we Develop a linear model using variables selected from Backward approach. The model will be built using the training sample of the data. The model will be validated using the validation sample of the data.

#####5.1.4.1 Split Validation

Split the data into training and validation samples. We will use (train.size)% for training and (100-train.size)% for validation

* set.seed(): Set the seed of R's random number generator, which is useful for creating simulations or random objects that can be reproduced.

We select 80% data for training and rest 20% for the testing and validation.
```{r, message=FALSE, warning=FALSE}
set.seed(2017)
train.size <- 0.8
train.index <- sample.int(length(merge.sel$Kwh),round(length(merge.sel$Kwh)*train.size))
train.sample <- merge.sel[train.index,]
train.val <- merge.sel[-train.index,]
```
`train.sample` contains our training data and `train.val` contains our remaining validation data.

Multiple regression model uses a simple formula:
     Kwh = B0 + B1xTemperatureF + B2xPeakHour + B3xDew_PointF

We will perform additional tests on training data. We will use a stepwise selection of variables by backward elimination. We will consider all possible candidate variables and eliminate one at a time.

* lm():lm is used to fit linear models. It can be used to carry out regression, single stratum analysis of variance and analysis of co-variance.

```{r, message=FALSE, warning=FALSE}
fit <- lm(Kwh ~ hour + month + Day.of.Week + weekday + PeakHour + TemperatureF + Dew_PointF +
                Humidity + Sea_Level_PressureIn + VisibilityMPH , data = train.sample)
```

By cheeking the summary of `fit` we would get to know the fit of our linear model.
```{r, message=FALSE, warning=FALSE}
summary(fit)
```

By plotting `fit` it provide checks for heteroscedasticity, normality, and influential observations.

Residuals vs Fitted:
This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and an outcome variable and the pattern could show up in this plot if the model doesn't capture the non-linear relationship.

Normal Q-Q:
This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? It's good if residuals are lined well on the straight dashed line.

Scale-Location:
It's also called Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). It's good if you see a horizontal line with equally (randomly) spread points.

Residuals vs Leverage:
This plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis (whatever outliers mean).

```{r, message=FALSE, warning=FALSE}
plot(fit)
```

Lets check the distribution of errors around the model using car library. 
* car library: It is used to compute Correlation and Linear Regression.
```{r, message=FALSE, warning=FALSE}
library(car)
```

Let us check the correlation and regression for `fit`.

```{r, message=FALSE, warning=FALSE}
car::crPlots(fit)
```

Red Line is the model and green line shows the fit of the variables.

Cook's distance is useful for identifying outliers in the X values (observations for predictor variables). It also shows the influence of each observation on the fitted response values. An observation with Cook's distance larger than three times the mean Cook's distance might be an outlier.

```{r, message=FALSE, warning=FALSE}
# Cook's D plot, cutoff as 4/(n-k-1)
cutoff <- 4/((nrow(train.sample)-length(fit$coefficients)-2))
# Identify D values > cutoff
plot(fit,which=4, cook.levels = cutoff) 
# Check, how important are these observations for the construction of the model
plot(fit,which=5, cook.levels = cutoff) 
```

After removing the not fitting values we will again check for fit our model.

```{r, message=FALSE, warning=FALSE}
fit <- lm(Kwh ~ hour + month + Day.of.Week + weekday + PeakHour + TemperatureF + Dew_PointF +
                Humidity + Sea_Level_PressureIn + VisibilityMPH , data = train.sample)
```

Checking the summary of `fit` again, we would get to know the new fit of our linear model.

```{r, message=FALSE, warning=FALSE}
summary(fit)
```

Also checking the correlation and regression for `fit`.

```{r, message=FALSE, warning=FALSE}
car::crPlots(fit)
```

Plotting Cook's D plot to recheck the fit.

```{r, message=FALSE, warning=FALSE}
# Cook's D plot, cutoff as 4/(n-k-1)
cutoff <- 4/((nrow(train.sample)-length(fit$coefficients)-2))
# Identify D values > cutoff
plot(fit,which=4, cook.levels = cutoff) 
# Check, how important are these observations for the construction of the model
plot(fit,which=5, cook.levels = cutoff) 
# By observing above plots, we remove the records which are indicated to be not fitting.
train.sample <- train.sample[-which(rownames(train.sample) %in% c("1062","117","8170")),]
```

Now one more time we are curious to know fit, so lets use lm()

```{r, message=FALSE, warning=FALSE}
fit <- lm(Kwh ~ hour + month + Day.of.Week + weekday + PeakHour + TemperatureF + Dew_PointF +                      Humidity + Sea_Level_PressureIn + VisibilityMPH , data = train.sample)
```

We print the final Print the Regression Coefficients. 
```{r, message=FALSE, warning=FALSE}
summary(fit) #R-squared:  0.5646,	Adjusted R-squared:  0.5639
```

We can see that we managed to increase adjusted r square value by 1% which is very insignificant.

Now we can evaluate the final linear model. We will find all predicted values for both a training and a validation set.

```{r, message=FALSE, warning=FALSE}

trainPred = subset(train.sample, select = -c(Kwh))
validPred = subset(train.val, select = -c(Kwh))
train.sample$Pred.Kwh <- predict(fit, newdata = trainPred)
train.val$Pred.Kwh <- predict(fit, newdata = validPred)
```

Check how good is the model on the training set ~ correlation^2,RME, MAE and MAPE

```{r, message=FALSE, warning=FALSE}
val.corr <- round(cor(train.sample$Pred.Kwh,train.sample$Kwh),2)
val.corr
```

Lets check the data on how exactly has it predicted the Kwh values.
```{r, message=FALSE, warning=FALSE}
head(train.sample[,c("Kwh","Pred.Kwh")])
```

As we can see there is high correlation between actual and predicted. Lets calculate other parameters for the model.
Note:Bringing back the transformed kwH with reverse transformation.

```{r, message=FALSE, warning=FALSE}
accuracy(10^train.val$Pred.Kwh, 10^train.val$Kwh)
write.csv(accuracy(10^train.val$Pred.Kwh, 10^train.val$Kwh), 
          file = "Part2/Outpout/PerformanceMetrics_split.csv")
```

We will also print coefficients of our model

```{r, message=FALSE, warning=FALSE}
coef(fit)
write.csv(coef(fit),file = "Part2/Outpout/RegressionOutputs.csv")
```

We have processed our model using split validation, we can use cross validation to check for better results.

#####5.1.4.2 Cross Validation
Cross-validation is one of the most widely-used method for model selection, and for choosing tuning parameter values. It calculates the estimated K-fold cross-validation prediction error for generalized linear models.

* caret library: The caret package (short for classification and regression training) contains functions to streamline the model training process for complex regression and classification problems.
* klaR library: Miscellaneous functions for classification and visualization.
```{r, message=FALSE, warning=FALSE}
library(caret)
library(klaR)
library(randomForest)
```

We set the 80% of data as train data `train.sample` and test data as `train.val`.

```{r, message=FALSE, warning=FALSE}
train.size <- 0.8
train.index <- sample.int(length(merge.sel$Kwh),round(length(merge.sel$Kwh)*train.size))
train.sample <- merge.sel[train.index,]
train.val <- merge.sel[-train.index,]
```

We now control the training data.

* trainControl(): To modify the resampling method, a trainControl function is used. K is controlled by the number argument and defaults to 10
```{r, message=FALSE, warning=FALSE}
train_control <- trainControl(method="cv", number=3)
```

Lets train the model now.

```{r, message=FALSE, warning=FALSE}
model <- train(Kwh ~ hour + month + Day.of.Week + weekday + PeakHour + TemperatureF + Dew_PointF +
                 Humidity + Sea_Level_PressureIn + VisibilityMPH , data = train.sample,
               trControl=train_control)
```

We summarize results to have look at our model.

```{r, message=FALSE, warning=FALSE}
print(model)
```

We now pridict the `Kwh` values using this model

```{r, message=FALSE, warning=FALSE}

trainPred = subset(train.sample, select = -c(Kwh))
validPred = subset(train.val, select = -c(Kwh))
train.sample$Pred.Kwh <- predict(model, newdata = trainPred)
train.val$Pred.Kwh <- predict(model, newdata = validPred)
```

We will now check the accuracy of our prediction.

```{r, message=FALSE, warning=FALSE}
accuracy(10^train.val$Pred.Kwh, 10^train.val$Kwh)
write.csv(accuracy(10^train.val$Pred.Kwh, 10^train.val$Kwh), 
          file = "Part2/Outpout/PerformanceMetrics_cross.csv")
```

We can see that accuracy has ben improved compared to split validation.


##6 Conclusion