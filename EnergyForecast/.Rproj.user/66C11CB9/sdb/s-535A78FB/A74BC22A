{
    "collab_server" : "",
    "contents" : "--- \ntitle: \"Boston Power Consumption\"\nauthor: \"Lalit, Lipsa, Sameer\"\ndate: \"October 25, 2016\"\noutput: \n  word_document: \n    keep_md: yes\n    toc: yes\n---\n\n\\pagebreak\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n##1. Introduction\nThe report introduces to the dataset of electricity consumption of the Mildred School of Boston Area.The primary purpose was to demonstrate a forecast model to predict energy usage of Boston city which can play an important role in the current and future electricity generation. We tried to infer the  relationship from various factors that potentially impacts the enery consumption in the city.\n\n###1.1 About the data\nThe data set that is take into consideration is derived from two sources: \n-one is the data collected in 2014 at Mildred School.\n-Second is the hourly weather data pulled from https://www.wunderground.com using API. \n\n###1.1 Mildred school energy usage dataset\nThe initial dataset contains Boston energy usage captured at every 5 mins interval for a particular account \"26908650026\" and channel \"MILDRED SCHOOL 1\".\n\n####1.1.1 Data description\nThe study data  was given in two flat files of 4MB each. We imported the datasets from our working directory as:\n```{r ,message=FALSE,warning=FALSE,echo=FALSE}\nsetwd(\"D:/Fall 16/ADS/Git Repo Assignemnt 2- Team 7/InputFiles\")\nrd1 <-  read.csv(\"rawData1.csv\",header = TRUE)\nrd2 <- read.csv(\"rawData2.csv\",header = TRUE)\n```\n\n####1.1.2 Description of rows and columns\nTo have an insight of the data, we took a closer look at the 4 variables `Account`, `Date`, `Channel` and `Units` recorded at every 5 min time interval.\n\n\nVariable      | Type          | Range        | Description\n------------- | ------------- |------------- |--------------------------------------------------------------\n`Account`     | Numeric       | 26908650026  | Account number belonging to Mildred School\n`Date`        | Date          | Jan-Dec'14   | Complete 2014 year data\n`Channel`     | Factor        | 7 Channels   | All Channels linked to the account, including                                                 Mildred School\n`Units`       | Factor        | 2 Units      | Units in which usage is recorded. kWh and kVARh \n\n###1.2 Dataset from wunderground.com\nThe second dataset can be fetched from the website containing weather data wunderground.com with the help of the \"weatherData\" package . The package gives all the weather related information taking location and date range as input.\n\n####1.2.1 Data description\nWe have extracted the  `Temperature`, `Dew_PointF`, `Humidity`, `Sea_Level_PressureIn`, `VisibilityMPH`, `Wind_Direction`, `Wind_SpeedMPH`, `Conditions` and `WindDirDegrees` data for Boston (KBOS) for 2014 Year.\n\n####1.2.2 Description of rows and columns\nThe weather data is closely understood by looking a columns and their charateristics.\n\nVariable               | Type      | Unit      | Range    | Description\n---------------------- | --------- |---------- | -------- |--------------------------------------------------------------\n`TemperatureF`         | Numeric   | Fahrenheit|  0 - 100 | Temprature in atmosphere\n\n`Dew_PointF`           | Numeric   | Fahrenheit|-20 -  80 | Temperature to which the air needs                                                             to be cooled to make the air                                                                  saturated\n\n`Humidity`             | Numeric   | Percent(%)| 10 - 100 | The water content of the air\n`Sea_Level_PressureIn` | Numeric   | Pascal    | 28 -  32 | Atmosphere exerts a pressure on                                                               the surface of the earth at sea                                                               level. \n`VisibilityMPH`        | Numeric   | Miles/Hour|  0 -  10 | Measure of the distance at which                                                              an object or light can be clearly                                                             discerned.\n`Wind_SpeedMPH`        | Numeric   | Miles/Hour|  0 -  50 | Wind speed in MPH\n`Conditions`           | Numeric   | Condition |    --    | state of atmosphere in temp,wind.\n`WindDirDegrees`       | Numeric   | Degrees   |  0 - 360 | In which direct wind is moving\n\n##2. Goal\n\nThe primary aim was to build and validate a forcasting model that will predict the energy usage in future years by computing two datasets. We  applied the multi-linear regression to model Power usage as a function of multiple variables. To supply this model we need to first do data wrangling, then Algorithm implementation and finally forcast.\n\n##3. Process Flow\nData wrangling -> Feature selection -> Algorithm Implemetation -> Forcasting Model -> Prediction\n\n###3.1 Installing Packages and libraries\nTo process the data we will import required packages and libraries\n\n####3.1.1 dplyr package\nWe will install and load the dplyr package that contains additional functions for data manipulation using data frames. It allows us to order rows, select rows, select variables, modify variables and summarize variables. We will be using functions like distinct, filter, group by, n_distinct from this package.\n```{r, message=FALSE,warning=FALSE}\nlibrary(dplyr)\n```\n\n####3.1.2 tidyr package\nIt's designed specifically for data tidying (not general reshaping or aggregating) and works well with 'dplyr' data pipelines.\n```{r, message=FALSE,warning=FALSE}\nlibrary(tidyr)\n```\n\n\n####3.1.3 lubridate package\nFunctions to work with date-times and time-spans: fast and user friendly parsing of date-time data, extraction and updating of components of a date-time (years, months, days, hours, minutes, and seconds), algebraic manipulation on date-time and time-span objects. The 'lubridate' package has a consistent and memorable syntax that makes working with dates easy and fun.\n```{r, message=FALSE,warning=FALSE}\nlibrary(lubridate)\n```\n\n####3.1.4 weatherData package\nFunctions that help in fetching weather data from websites. Given a location and a date range, these functions help fetch weather data (temperature, pressure etc.) for any weather related analysis. \n```{r, message=FALSE,warning=FALSE}\nlibrary(weatherData)\n```\ninstall.packages(\"weatherData\")\n\n###3.2 Data wrangling and prepration\nData munging or data wrangling is loosely the process of manually converting or mapping data from one \"raw\" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools.\n\n####3.2.1 Primary dataset filteration\nAs the dataset contains channels 7 different channels and we are interested in only kwh units for Mildred School\nwe will do intial dataset filtering and merging.\n\n* filter(): Applies linear filtering to a univariate time series or to each series separately of a multivariate time series.\n* rbind(): Take a sequence of vector, matrix or data-frame arguments and combine by columns or rows, respectively. \n* \n```{r ,message=FALSE,warning=FALSE}\n# Filtering\nrd1 <-  filter(rd1, Units == \"kWh\",Channel  == \"MILDRED SCHOOL 1\")\nrd2 <-  filter(rd2, Units == \"kWh\",Channel  == \"MILDRED SCHOOL 1\")\n\n# Merging\ndata <- rbind(rd1,rd2) \n```\n####3.2.2 Datatype conformity\nAs we read the data from dataset, r intutively understands datatypes. We need to check structure of our data and then change the columns structure according to our requirements.\n\n`str()` function gives us the snapshot of datatypes of all the columns\n```{r ,message=FALSE,warning=FALSE}\nstr(data[1:4])\n```\n\nWe noticed that the Date , Channel and Units are Factor variables with levels .We converted the datatype of these columns in to as.Date() and as.charcter() function which were available in the base r library.\n\n```{r ,message=FALSE,warning=FALSE}\ndata$Date <- as.Date(data$Date,\"%m/%d/%Y\")\ndata$Channel <- as.character(data$Channel)\ndata$Units <- as.character(data$Units)\n\n# Re-check structure of data\nstr(data[1:4])\n```\n\nAs we can see that the data is widely spread and for any analysis to be done we need to convert the data into long format. We will first find the poisitions of the columns and also save the columnnames into array.\n\n* grep(): Search for matches to argument pattern within each element of a character vector\n```{r,message=FALSE,warning=FALSE}\ncolumn_Pos <- grep(\"X\",colnames(data))\n```\n\nThis wills store the names of the columns for the observations\n```{r,message=FALSE,warning=FALSE}\ncolumnnamesval <- grep(\"X\",colnames(data),value = TRUE)\n```\n###3.3 Data Gathering\nWe gathered the valid data to make it fit for analysis.\n\n####3.3.1 Gathering the data  in long format.\nThough we found the data was spread with theelectricity consumption value spread out across columns.We tried to reformat the data such that these common attributes are together into a single value\n\n* gather() :takes the column value and collapses them in to key-value pairs.\n* head() : Returns the first parts of a vector, matrix, table, data frame or function.\n\n```{r,message=FALSE,warning=FALSE}\nData.long <- gather(data,key,values,column_Pos)\nhead(Data.long)\n```\n\n####3.3.2 Derive data\nNow we have the data in the long format we can apply some aggregations on the data for each hour which will comprise of 12 observations for each hour. We will create 24 new columns which will be sum of 12 observations of each hour using dplyr summarize and group by function.\n\nColumns to be derived:\n\nColumn Name   | Description\n------------- | ------------------------------------------------------------\n`kWh`         | Sum of 12 observations (5 min intervals rolled up to hourly)\n`month`       | 1-12 => Jan-Dec - Derived from dates\n`day`         | 1-31 - Derived from dates\n`year`        | Derived from dates\n`hour`        | 0-23 - Derived for each record corresponding to the hour of observation\n`Day of Week` | 0-6 -Sun-Sat - Derived from dates\n`Weekday`     | 1- Yes 0- No - Derived from dates\n`Peakhour`    | 7AM-7PM - 1 ; 7PM-7AM - 0\n\nBelow is a user defined function to find the Weekday\n\n```{r,message=FALSE,warning=FALSE}\ncheckWeekday <- function(date){\n  a = if((wday(date) == 6 || wday(date) == 7)) 0 else 1\n  return(a)\n}\n```\n\nWe Will use dplyr chaining function to group the data first and summarize the columns for each hour using the 12 observations/each hour\n\n* group_by():  It breaks down a dataset into specified groups of rows. \n* summarise(): Summarise multiple values to a single value.\n* mutate(): Mutate adds new variables and preserves existing; transmute drops existing variables.\n\n```{r,message=FALSE,warning=FALSE}\naggData <- Data.long %>% group_by(Account,Date,Channel,Units) %>% \n  summarise('0' = sum(values[key %in% columnnamesval[1:12]]),\n            '1' = sum(values[key %in% columnnamesval[13:24]]),\n            '2' = sum(values[key %in% columnnamesval[25:36]]),\n            '3' = sum(values[key %in% columnnamesval[37:48]]),\n            '4'  = sum(values[key %in% columnnamesval[49:60]]),\n            '5' = sum(values[key %in% columnnamesval[61:72]]),\n            '6' = sum(values[key %in% columnnamesval[73:84]]),\n            '7' = sum(values[key %in% columnnamesval[85:96]]),\n            '8' = sum(values[key %in% columnnamesval[97:108]]),\n            '9' = sum(values[key %in% columnnamesval[109:120]]),\n            '10' = sum(values[key %in% columnnamesval[121:132]]),\n            '11' = sum(values[key %in% columnnamesval[133:144]]),\n            '12' = sum(values[key %in% columnnamesval[145:156]]),\n            '13' = sum(values[key %in% columnnamesval[157:168]]),\n            '14' = sum(values[key %in% columnnamesval[169:180]]),\n            '15' = sum(values[key %in% columnnamesval[181:192]]),\n            '16' = sum(values[key %in% columnnamesval[193:204]]),\n            '17' = sum(values[key %in% columnnamesval[205:216]]),\n            '18' = sum(values[key %in% columnnamesval[217:228]]),\n            '19' = sum(values[key %in% columnnamesval[229:240]]),\n            '20' = sum(values[key %in% columnnamesval[241:252]]),\n            '21' = sum(values[key %in% columnnamesval[253:264]]),\n            '22' = sum(values[key %in% columnnamesval[265:276]]),\n            '23' = sum(values[key %in% columnnamesval[277:288]])) %>%\n  mutate(month = lubridate::month(Date),day = day(Date),year = year(Date),'Day of Week' = wday(Date)) %>%\n  mutate(weekday = sapply(Date, function(x) checkWeekday(x)))\n```\n\nGathering the column again in long format so that data is in clean consistent format. This way we have the kWH value for every hour and for every day of the year 2014\n```{r,message=FALSE,warning=FALSE}\naggData.long <- gather(aggData,hour,Kwh,5:28)\n```\n\nWe will change the hour column into numeric\n```{r,message=FALSE,warning=FALSE}\naggData.long$hour <- as.numeric(aggData.long$hour)\n```\n\nNow lets derive peak hour. Using the table definition we can define a function based on hours like below\n```{r,message=FALSE,warning=FALSE}\ncal_PeakHour <- function(hour){\n  p = if(hour > 6 & hour < 20) 1 else 0\n  return(p)\n}\n```\n* sapply(): Each element of which is the result of applying FUN to the corresponding element of X.\n```{r,message=FALSE,warning=FALSE}\naggData.long$PeakHour <- sapply(aggData.long$hour,function(x) cal_PeakHour(x))\n```\nLets have a look at the structure of our dataset now\n```{r,message=FALSE,warning=FALSE}\nstr(aggData.long)\n```\nThe structure seems to align with our requirement now lets pull up the other dataset\n\n####3.3.3 Data from weather API\nWe have used the WeatherData Package to pull all the weather related information from wunderground.com.The weatherData package takes a date range and Location as an input  .\n\nWe first calculated the minium and max date for our observed dataset\n```{r,message=FALSE,warning=FALSE}\nmindate <- min(aggData.long$Date)\nmaxdate <- max(aggData.long$Date)\n```\n\nConverted the date range in to a desired format\n```{r,message=FALSE,warning=FALSE}\nmindate <- as.Date(\"01/01/2014\", \"%m/%d/%Y\")\nmaxdate <- as.Date(\"12/31/2014\", \"%m/%d/%Y\")\n```\n\nWe got the station code for Boston \n```{r,message=FALSE,warning=FALSE}\ngetStationCode(\"Boston\")\n```\n\nThe station_id for Boston is \"KBOS\"\n\n* getWeatherForDate(): Getting data for a range of dates, it has certain parameters\n* `station_id`: is a valid 3- or 4-letter Airport code or a valid Weather Station ID (example: \"KBOS\" for Boston).\n* `start_date`: string representing a date in the past (\"YYYY-MM-DD\", all numeric)\n* `end_date`  : If an interval is to be specified,end_date is a strin grepresenting a date in the past (\"YYYY-MM-DD\", all numeric) and greater than the start date\n* `opt_detailed`:indicates if detailed records for the station are desired. (default FALSE). By default only one records per date is returned.\n* `opt_custom_columns`:  to indicate if only a user-specified set of columns are to be returned. (default FALSE) If TRUE, then the desired columns must be specified via custom_columns \n* `custom_columns`: Vector of integers specified by the user to indicate which columns to fetch. The Date column is always returned as the first column.\n\nOnce we fetched the respective inputs for the WeatherData .We tried to extract the weather information witht the applied inputs.\n\n```{r,message=FALSE,warning=FALSE}\n#WeatherData <- getWeatherForDate(\"KBOS\", start_date=mindate,\n#                                 end_date = maxdate,\n#                                 opt_detailed=T,opt_custom_columns=T,\n#                                 custom_columns=c(2:13))\n```\n\n\n```{r,message=FALSE,warning=FALSE,echo=FALSE}\nWeatherData <- read.csv(\"WeatherData.csv\",header = TRUE)\n```\n\n##4.Data Analysis\n\n###4.1 Insight on Weather Data\n\n```{r,message=FALSE,warning=FALSE}\n#head(WeatherData)\n```\n\nWe calculated the date and hour using the \"Lubricate\" package we have used.\n\n```{r,message=FALSE,warning=FALSE}\n#WeatherData$date = date(WeatherData$Time)\n#WeatherData$hour = hour(WeatherData$Time)\n```\n\n```{r,message=FALSE,warning=FALSE}\n#View(WeatherData)\n```\n\nAfter looking in to the information pulled by the WeatherData package ,we got a picture that data is spread on hourly interval. We tried to confirm with the following function.\n\n```{r,message=FALSE,warning=FALSE}\ntable(WeatherData$date)\n```\n\nAfter looking at the tabular values, we deduced that although most of the days had 24 observations, some of them have more than 24 .\n\nThe details revealed that in some instances observations were taken more than once for each hour,as illustrated in the following case :\n```{r,message=FALSE,warning=FALSE}\nView(WeatherData[which(WeatherData$date == \"2014-06-05\"),])\n```\n\nDetail Observation :\n\n* we  got -999999 value in columns TempratureF, DewPointF, Sea_Level_PressureIn, Visibility MPH\n* We converted the data to the respective data types\n* WindSpeed \"Calm\" which mean 0: Converting to character as it is in factor\n\n```{r,message=FALSE,warning=FALSE}\nWeatherData$date <-  as.Date(WeatherData$date,\"%m/%d/%Y\")\nWeatherData$TemperatureF <- as.numeric(WeatherData$TemperatureF)\nWeatherData$Dew_PointF <- as.numeric(WeatherData$Dew_PointF)\nWeatherData$Sea_Level_PressureIn <- as.numeric(WeatherData$Sea_Level_PressureIn)\nWeatherData$VisibilityMPH <- as.numeric(WeatherData$VisibilityMPH)\nWeatherData$WindDirDegrees <- as.numeric(WeatherData$WindDirDegrees)\nWeatherData$Humidity <- as.numeric(WeatherData$Humidity)\n\nWeatherData$Wind_SpeedMPH[WeatherData$Wind_SpeedMPH == \"Calm\"] <- 0\nWeatherData$Wind_SpeedMPH <- as.numeric(WeatherData$Wind_SpeedMPH)\n```\n\n We need our data to fall in normal range to remove outliers\n \n                       MIN   MAX\n*    TempF\t                0\t  100\n*    DewPointF\t          -20\t  80\n*    Humidity\t           10\t  100\n*    Sea_Level_Pressure\t 28\t  32\n*    Visibility\t          0   10\n*    Wind_Speed\t          0 \t50\n\n###4.2 Handling Outliers\n* We used the approach of subsituting the previous or the next value of the observation.\n  For example, if the record 8999 has Temperature as -9999 we  used the record of 8998 so that   this is still acceptable.\n\n* We tried to handle the outliers with the following function\n\n```{r,message=FALSE,warning=FALSE}\nremove_out <- function(param,index,min_v,max_v)\n{\n  val = NULL\n  val = param[index]\n  \n  if(val < min_v | val > max_v | is.na(val)){\n    if(index-1 >= 1){\n      val = param[index-1]\n    } else if (index-1 <= 0){\n      val = param[index+1]\n    } \n    return(val)\n  } else{\n    print(\"Nothing changed\")\n    return(val)  #Normal Value return\n  }\n}\n\n```\n\nWith the above function  removed the outliers for Temperature. We found out the records where Temperature is falling out of the range defined in the table\n\n\n Temperature\n \n```{r,message=FALSE,warning=FALSE}\nindex <- which(WeatherData$TemperatureF < 0 | WeatherData$TemperatureF > 100 | is.na(WeatherData$Dew_PointF))\nprint(index)\n```\n\n\nWe had an insight in to the data records\nWeatherData[8206,]\n\n\n We found that it was indeed an outlier,could be a machine input error. We tried to remove this implementing the function and checked the record again after the function \n```{r,message=FALSE,warning=FALSE}\nfor (i in index){\nWeatherData$TemperatureF[i] = remove_out(WeatherData$TemperatureF,i,0,100)\n}\nWeatherData[8206,]\n```\n\nWe were successful in getting in to shape.We implemtened the same thing for the other features:\n\n Dew Point\n```{r,message=FALSE,warning=FALSE}\nindex <- which(WeatherData$Dew_PointF < -20 | WeatherData$Dew_PointF > 80 | is.na(WeatherData$Dew_PointF))\nfor (i in index){\n  WeatherData$Dew_PointF[i] = remove_out(WeatherData$Dew_PointF,i,-20,80)\n}\n```\n\n Humidity\n```{r,message=FALSE,warning=FALSE}\nindex <- which(WeatherData$Humidity < 10 | WeatherData$Humidity > 100 | is.na(WeatherData$Humidity))\nfor (i in index){\n  WeatherData$Humidity[i] = remove_out(WeatherData$Humidity,i,10,100)\n}\n```\n\n Wind_SpeedMPH\n```{r,message=FALSE,warning=FALSE}\nindex <- which(WeatherData$Wind_SpeedMPH < 0 | WeatherData$Wind_SpeedMPH > 50 | is.na(WeatherData$Wind_SpeedMPH))\nfor (i in index){\n  WeatherData$Wind_SpeedMPH[i] = remove_out(WeatherData$Wind_SpeedMPH,i,0,50)\n}\n```\n\nSea_Level_Pressure\n```{r,message=FALSE,warning=FALSE}\nindex <- which(WeatherData$Sea_Level_PressureIn < 28 | WeatherData$Sea_Level_PressureIn > 32 | is.na(WeatherData$Sea_Level_PressureIn))\nfor (i in index){\n  WeatherData$Sea_Level_PressureIn[i] = remove_out(WeatherData$Sea_Level_PressureIn,i,28,32)\n}\n```\n\n VisibilityMPH\n```{r,message=FALSE,warning=FALSE}\nindex <- which(WeatherData$VisibilityMPH < 0 | WeatherData$VisibilityMPH > 10 | is.na(WeatherData$VisibilityMPH))\nfor (i in index){\n  WeatherData$VisibilityMPH[i] = remove_out(WeatherData$VisibilityMPH,i,0,10)\n}\n```\n\n WindDirDegree\n```{r,message=FALSE,warning=FALSE}\nindex <- which(WeatherData$WindDirDegrees < 0 | WeatherData$WindDirDegrees > 360 | is.na(WeatherData$WindDirDegrees))\nfor (i in index){\n  WeatherData$WindDirDegrees[i] = remove_out(WeatherData$WindDirDegrees,i,0,360)\n}\n```\n\n The data was clean and consistent. We aggregated the dataset as done in part 1, so that we get records for each hour and we can take average values for numeric values and frequency count for character values.\nWe followed the below steps:\n 1) Remove non essential features like Time, Gust_speedMH,P,E\n 2) Group the data by Date and hour \n 3) summarise base on mean and frequency count\n\n\n```{r,message=FALSE,warning=FALSE}\nWeatherData.Agg <- WeatherData %>% \n  select(-c(Time,Gust_SpeedMPH,\n            PrecipitationIn,Events)) %>%\n  group_by(date,hour) %>% \n  summarise(TemperatureF = mean(TemperatureF),\n                                    Dew_PointF = mean(Dew_PointF),\n                                    Humidity = mean(Humidity),\n                                    Sea_Level_PressureIn = mean(Sea_Level_PressureIn),\n                                    VisibilityMPH = mean (VisibilityMPH),\n                                    Wind_SpeedMPH = mean(Wind_SpeedMPH),\n                                    WindDirDegrees = mean(WindDirDegrees),\n            Conditions = names(table(Conditions))[which.max(table(Conditions))],\n            Wind_Direction = names(table(Wind_Direction))[which.max(table(Wind_Direction))])\n\nhead(WeatherData.Agg)\n```\n\nOnce We had both the dataset wit  in the desired format\nWe merged the data with part 1 of the energy usage data by Date and hour\n\n###4.3 Final Output Data\n```{r,message=FALSE,warning=FALSE}\nmergeData <- merge(aggData.long,WeatherData.Agg,by.x = c(\"Date\",\"hour\"),by.y = c(\"date\",\"hour\"))\nhead(mergeData)\n\n```\n\nArranging the data by Date and hour\n```{r,message=FALSE,warning=FALSE}\nmergeData<- arrange(mergeData,Date,hour)\nhead(mergeData)\n```\n\nwrite.csv(mergeData,\"MergedData.csv\")\n\n\n##5: Applying the algorithm\nTesting datahttp://r-statistics.co/Linear-Regression.html\ncor(mergeData$Kwh,mergeData$PeakHour)\nplot(mergeData$Kwh,mergeData$PeakHour)\nscatter.smooth(x=mergeData$Kwh, y=mergeData$PeakHour, main=\"Kwh ~ PeakHour\")\nlinearMod <- lm(Kwh ~ PeakHour, data = mergeData)\nsummary(linearMod)\nprint(linearMod)\n\n",
    "created" : 1477701736037.000,
    "dirty" : false,
    "encoding" : "ISO8859-1",
    "folds" : "",
    "hash" : "2370524203",
    "id" : "A74BC22A",
    "lastKnownWriteTime" : 1477698333,
    "last_content_update" : 1477698333,
    "path" : "C:/Users/Samy/Dropbox/ADS/Assignment2/Knitted_File.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}