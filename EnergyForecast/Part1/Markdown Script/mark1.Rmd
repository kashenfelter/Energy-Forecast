---
title: "Boston Electricity"
author: "Sameer, Lalit, Lipsa"
date: "October 25, 2016"
output:
  word_document:
    keep_md: yes
    toc: yes
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

\pagebreak

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##1. Introduction
The report introduces to the dataset of electricity consumption of the Mildred School of Boston Area.The primary purpose was to demonstrate a forecast model to predict energy usage of Boston city which can play an important role in the current and future electricity generation. We tried to infer the  relationship from various factors that potentially impacts the enery consumption in the city.

###1.1 About the data
The data set that is take into consideration is derived from two sources: 
-one is the data collected in 2014 at Mildred School.
-Second is the hourly weather data pulled from https://www.wunderground.com using API. 

###1.1 Mildred school energy usage dataset
The initial dataset contains Boston energy usage captured at every 5 mins interval for a particular account "26908650026" and channel "MILDRED SCHOOL 1".

####1.1.1 Data description
The study data  was given in two flat files of 4MB each. We imported the datasets from our working directory as:
```{r ,message=FALSE,warning=FALSE,echo=FALSE}
setwd("D:/Fall 16/ADS/Git Repo Assignemnt 2- Team 7/InputFiles")
rd1 <-  read.csv("rawData1.csv",header = TRUE)
rd2 <- read.csv("rawData2.csv",header = TRUE)
```

####1.1.2 Description of rows and columns
To have an insight of the data, we took a closer look at the 4 variables `Account`, `Date`, `Channel` and `Units` recorded at every 5 min time interval.


Variable      | Type          | Range        | Description
------------- | ------------- |------------- |--------------------------------------------------------------
`Account`     | Numeric       | 26908650026  | Account number belonging to Mildred School
`Date`        | Date          | Jan-Dec'14   | Complete 2014 year data
`Channel`     | Factor        | 7 Channels   | All Channels linked to the account, including                                                 Mildred School
`Units`       | Factor        | 2 Units      | Units in which usage is recorded. kWh and kVARh 

###1.2 Dataset from wunderground.com
The second dataset can be fetched from the website containing weather data wunderground.com with the help of the "weatherData" package . The package gives all the weather related information taking location and date range as input.

####1.2.1 Data description
We have extracted the  `Temperature`, `Dew_PointF`, `Humidity`, `Sea_Level_PressureIn`, `VisibilityMPH`, `Wind_Direction`, `Wind_SpeedMPH`, `Conditions` and `WindDirDegrees` data for Boston (KBOS) for 2014 Year.

####1.2.2 Description of rows and columns
The weather data is closely understood by looking a columns and their charateristics.

Variable               | Type      | Unit      | Range    | Description
---------------------- | --------- |---------- | -------- |--------------------------------------------------------------
`TemperatureF`         | Numeric   | Fahrenheit|  0 - 100 | Temprature in atmosphere

`Dew_PointF`           | Numeric   | Fahrenheit|-20 -  80 | Temperature to which the air needs                                                             to be cooled to make the air                                                                  saturated

`Humidity`             | Numeric   | Percent(%)| 10 - 100 | The water content of the air
`Sea_Level_PressureIn` | Numeric   | Pascal    | 28 -  32 | Atmosphere exerts a pressure on                                                               the surface of the earth at sea                                                               level. 
`VisibilityMPH`        | Numeric   | Miles/Hour|  0 -  10 | Measure of the distance at which                                                              an object or light can be clearly                                                             discerned.
`Wind_SpeedMPH`        | Numeric   | Miles/Hour|  0 -  50 | Wind speed in MPH
`Conditions`           | Numeric   | Condition |    --    | state of atmosphere in temp,wind.
`WindDirDegrees`       | Numeric   | Degrees   |  0 - 360 | In which direct wind is moving

##2. Goal

The primary aim was to build and validate a forcasting model that will predict the energy usage in future years by computing two datasets. We  applied the multi-linear regression to model Power usage as a function of multiple variables. To supply this model we need to first do data wrangling, then Algorithm implementation and finally forcast.

##3. Process Flow
Data wrangling -> Feature selection -> Algorithm Implemetation -> Forcasting Model -> Prediction

###3.1 Installing Packages and libraries
To process the data we will import required packages and libraries

####3.1.1 dplyr package
We will install and load the dplyr package that contains additional functions for data manipulation using data frames. It allows us to order rows, select rows, select variables, modify variables and summarize variables. We will be using functions like distinct, filter, group by, n_distinct from this package.
```{r, message=FALSE,warning=FALSE}
library(dplyr)
```

####3.1.2 tidyr package
It's designed specifically for data tidying (not general reshaping or aggregating) and works well with 'dplyr' data pipelines.
```{r, message=FALSE,warning=FALSE}
library(tidyr)
```


####3.1.3 lubridate package
Functions to work with date-times and time-spans: fast and user friendly parsing of date-time data, extraction and updating of components of a date-time (years, months, days, hours, minutes, and seconds), algebraic manipulation on date-time and time-span objects. The 'lubridate' package has a consistent and memorable syntax that makes working with dates easy and fun.
```{r, message=FALSE,warning=FALSE}
library(lubridate)
```

####3.1.4 weatherData package
Functions that help in fetching weather data from websites. Given a location and a date range, these functions help fetch weather data (temperature, pressure etc.) for any weather related analysis. 
```{r, message=FALSE,warning=FALSE}
library(weatherData)
```
install.packages("weatherData")

###3.2 Data wrangling and prepration
Data munging or data wrangling is loosely the process of manually converting or mapping data from one "raw" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools.

####3.2.1 Primary dataset filteration
As the dataset contains channels 7 different channels and we are interested in only kwh units for Mildred School
we will do intial dataset filtering and merging.

* filter(): Applies linear filtering to a univariate time series or to each series separately of a multivariate time series.
* rbind(): Take a sequence of vector, matrix or data-frame arguments and combine by columns or rows, respectively. 
* 
```{r ,message=FALSE,warning=FALSE}
# Filtering
rd1 <-  filter(rd1, Units == "kWh",Channel  == "MILDRED SCHOOL 1")
rd2 <-  filter(rd2, Units == "kWh",Channel  == "MILDRED SCHOOL 1")

# Merging
data <- rbind(rd1,rd2) 
```
####3.2.2 Datatype conformity
As we read the data from dataset, r intutively understands datatypes. We need to check structure of our data and then change the columns structure according to our requirements.

`str()` function gives us the snapshot of datatypes of all the columns
```{r ,message=FALSE,warning=FALSE}
str(data[1:4])
```

We noticed that the Date , Channel and Units are Factor variables with levels .We converted the datatype of these columns in to as.Date() and as.charcter() function which were available in the base r library.

```{r ,message=FALSE,warning=FALSE}
data$Date <- as.Date(data$Date,"%m/%d/%Y")
data$Channel <- as.character(data$Channel)
data$Units <- as.character(data$Units)

# Re-check structure of data
str(data[1:4])
```

As we can see that the data is widely spread and for any analysis to be done we need to convert the data into long format. We will first find the poisitions of the columns and also save the columnnames into array.

* grep(): Search for matches to argument pattern within each element of a character vector
```{r,message=FALSE,warning=FALSE}
column_Pos <- grep("X",colnames(data))
```

This wills store the names of the columns for the observations
```{r,message=FALSE,warning=FALSE}
columnnamesval <- grep("X",colnames(data),value = TRUE)
```
###3.3 Data Gathering
We gathered the valid data to make it fit for analysis.

####3.3.1 Gathering the data  in long format.
Though we found the data was spread with theelectricity consumption value spread out across columns.We tried to reformat the data such that these common attributes are together into a single value

* gather() :takes the column value and collapses them in to key-value pairs.
* head() : Returns the first parts of a vector, matrix, table, data frame or function.

```{r,message=FALSE,warning=FALSE}
Data.long <- gather(data,key,values,column_Pos)
head(Data.long)
```

####3.3.2 Derive data
Now we have the data in the long format we can apply some aggregations on the data for each hour which will comprise of 12 observations for each hour. We will create 24 new columns which will be sum of 12 observations of each hour using dplyr summarize and group by function.

Columns to be derived:

Column Name   | Description
------------- | ------------------------------------------------------------
`kWh`         | Sum of 12 observations (5 min intervals rolled up to hourly)
`month`       | 1-12 => Jan-Dec - Derived from dates
`day`         | 1-31 - Derived from dates
`year`        | Derived from dates
`hour`        | 0-23 - Derived for each record corresponding to the hour of observation
`Day of Week` | 0-6 -Sun-Sat - Derived from dates
`Weekday`     | 1- Yes 0- No - Derived from dates
`Peakhour`    | 7AM-7PM - 1 ; 7PM-7AM - 0

Below is a user defined function to find the Weekday

```{r,message=FALSE,warning=FALSE}
checkWeekday <- function(date){
  a = if((wday(date) == 6 || wday(date) == 7)) 0 else 1
  return(a)
}
```

We Will use dplyr chaining function to group the data first and summarize the columns for each hour using the 12 observations/each hour

* group_by():  It breaks down a dataset into specified groups of rows. 
* summarise(): Summarise multiple values to a single value.
* mutate(): Mutate adds new variables and preserves existing; transmute drops existing variables.

```{r,message=FALSE,warning=FALSE}
aggData <- Data.long %>% group_by(Account,Date,Channel,Units) %>% 
  summarise('0' = sum(values[key %in% columnnamesval[1:12]]),
            '1' = sum(values[key %in% columnnamesval[13:24]]),
            '2' = sum(values[key %in% columnnamesval[25:36]]),
            '3' = sum(values[key %in% columnnamesval[37:48]]),
            '4'  = sum(values[key %in% columnnamesval[49:60]]),
            '5' = sum(values[key %in% columnnamesval[61:72]]),
            '6' = sum(values[key %in% columnnamesval[73:84]]),
            '7' = sum(values[key %in% columnnamesval[85:96]]),
            '8' = sum(values[key %in% columnnamesval[97:108]]),
            '9' = sum(values[key %in% columnnamesval[109:120]]),
            '10' = sum(values[key %in% columnnamesval[121:132]]),
            '11' = sum(values[key %in% columnnamesval[133:144]]),
            '12' = sum(values[key %in% columnnamesval[145:156]]),
            '13' = sum(values[key %in% columnnamesval[157:168]]),
            '14' = sum(values[key %in% columnnamesval[169:180]]),
            '15' = sum(values[key %in% columnnamesval[181:192]]),
            '16' = sum(values[key %in% columnnamesval[193:204]]),
            '17' = sum(values[key %in% columnnamesval[205:216]]),
            '18' = sum(values[key %in% columnnamesval[217:228]]),
            '19' = sum(values[key %in% columnnamesval[229:240]]),
            '20' = sum(values[key %in% columnnamesval[241:252]]),
            '21' = sum(values[key %in% columnnamesval[253:264]]),
            '22' = sum(values[key %in% columnnamesval[265:276]]),
            '23' = sum(values[key %in% columnnamesval[277:288]])) %>%
  mutate(month = lubridate::month(Date),day = day(Date),year = year(Date),'Day of Week' = wday(Date)) %>%
  mutate(weekday = sapply(Date, function(x) checkWeekday(x)))
```

Gathering the column again in long format so that data is in clean consistent format. This way we have the kWH value for every hour and for every day of the year 2014
```{r,message=FALSE,warning=FALSE}
aggData.long <- gather(aggData,hour,Kwh,5:28)
```

We will change the hour column into numeric
```{r,message=FALSE,warning=FALSE}
aggData.long$hour <- as.numeric(aggData.long$hour)
```

Now lets derive peak hour. Using the table definition we can define a function based on hours like below
```{r,message=FALSE,warning=FALSE}
cal_PeakHour <- function(hour){
  p = if(hour > 6 & hour < 20) 1 else 0
  return(p)
}
```
* sapply(): Each element of which is the result of applying FUN to the corresponding element of X.
```{r,message=FALSE,warning=FALSE}
aggData.long$PeakHour <- sapply(aggData.long$hour,function(x) cal_PeakHour(x))
```
Lets have a look at the structure of our dataset now
```{r,message=FALSE,warning=FALSE}
str(aggData.long[1:12])
```
The structure seems to align with our requirement now lets pull up the other dataset

####3.3.3 Data from weather API
We have used the WeatherData Package to pull all the weather related information from wunderground.com.The weatherData package takes a date range and Location as an input  .

We first calculated the minium and max date for our observed dataset
```{r,message=FALSE,warning=FALSE}
mindate <- min(aggData.long$Date)
maxdate <- max(aggData.long$Date)
```

Converted the date range in to a desired format
```{r,message=FALSE,warning=FALSE}
mindate <- as.Date(mindate, "%m/%d/%Y")
maxdate <- as.Date(maxdate, "%m/%d/%Y")
```

We got the station code for Boston 
```{r,message=FALSE,warning=FALSE}
getStationCode("Boston")
```

The station_id for Boston is "KBOS"

* getWeatherForDate(): Getting data for a range of dates, it has certain parameters
* `station_id`: is a valid 3- or 4-letter Airport code or a valid Weather Station ID (example: "KBOS" for Boston).
* `start_date`: string representing a date in the past ("YYYY-MM-DD", all numeric)
* `end_date`  : If an interval is to be specified,end_date is a strin grepresenting a date in the past ("YYYY-MM-DD", all numeric) and greater than the start date
* `opt_detailed`:indicates if detailed records for the station are desired. (default FALSE). By default only one records per date is returned.
* `opt_custom_columns`:  to indicate if only a user-specified set of columns are to be returned. (default FALSE) If TRUE, then the desired columns must be specified via custom_columns 
* `custom_columns`: Vector of integers specified by the user to indicate which columns to fetch. The Date column is always returned as the first column.

Once we fetched the respective inputs for the WeatherData .We tried to extract the weather information witht the applied inputs.

```{r,message=FALSE,warning=FALSE}
WeatherData <- getWeatherForDate("KBOS", start_date=mindate,
                                 end_date = maxdate,
                                 opt_detailed=T,opt_custom_columns=T,
                                 custom_columns=c(2:13))
```

##4.Data Analysis

###4.1 Insight on Weather Data

```{r,message=FALSE,warning=FALSE}
head(WeatherData)
```

We calculated the date and hour using the "Lubricate" package we have used.

```{r,message=FALSE,warning=FALSE}
WeatherData$date = date(WeatherData$Time)
WeatherData$hour = hour(WeatherData$Time)
```

After looking in to the information pulled by the WeatherData package ,we got a picture that data is spread on hourly interval. We tried to confirm with the following function.

```{r,message=FALSE,warning=FALSE}
head(table(WeatherData$date))
```

After looking at the tabular values, we deduced that although most of the days had 24 observations, some of them have more than 24 .

The details revealed that in some instances observations were taken more than once for each hour,as illustrated in the following case :
```{r,message=FALSE,warning=FALSE}
View(WeatherData[which(WeatherData$date == "2014-06-05"),])
```

Detail Observation :

* we  got -999999 value in columns TempratureF, DewPointF, Sea_Level_PressureIn, Visibility MPH
* We converted the data to the respective data types
* WindSpeed "Calm" which mean 0: Converting to character as it is in factor

```{r,message=FALSE,warning=FALSE}
WeatherData$date <-  as.Date(WeatherData$date,"%m/%d/%Y")
WeatherData$TemperatureF <- as.numeric(WeatherData$TemperatureF)
WeatherData$Dew_PointF <- as.numeric(WeatherData$Dew_PointF)
WeatherData$Sea_Level_PressureIn <- as.numeric(WeatherData$Sea_Level_PressureIn)
WeatherData$VisibilityMPH <- as.numeric(WeatherData$VisibilityMPH)
WeatherData$WindDirDegrees <- as.numeric(WeatherData$WindDirDegrees)
WeatherData$Humidity <- as.numeric(WeatherData$Humidity)

WeatherData$Wind_SpeedMPH[WeatherData$Wind_SpeedMPH == "Calm"] <- 0
WeatherData$Wind_SpeedMPH <- as.numeric(WeatherData$Wind_SpeedMPH)
```

 We need our data to fall in normal range to remove outliers

###4.2 Handling Outliers
* We used the approach of subsituting the previous or the next value of the observation.
  For example, if the record 8999 has Temperature as -9999 we  used the record of 8998 so that   this is still acceptable.

* We tried to handle the outliers with the following function

```{r,message=FALSE,warning=FALSE}
remove_out <- function(param,index,min_v,max_v)
{
  val = NULL
  val = param[index]
  
  if(val < min_v | val > max_v | is.na(val)){
    if(index-1 >= 1){
      val = param[index-1]
    } else if (index-1 <= 0){
      val = param[index+1]
    } 
    return(val)
  } else{
    print("Nothing changed")
    return(val)  #Normal Value return
  }
}

```

With the above function  removed the outliers for Temperature. We found out the records where Temperature is falling out of the range defined in the table


 Temperature
 
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$TemperatureF < 0 | WeatherData$TemperatureF > 100 | is.na(WeatherData$Dew_PointF))
print(index)
```


We had an insight in to the data records
WeatherData[8206,]


 We found that it was indeed an outlier,could be a machine input error. We tried to remove this implementing the function and checked the record again after the function 
```{r,message=FALSE,warning=FALSE}
for (i in index){
WeatherData$TemperatureF[i] = remove_out(WeatherData$TemperatureF,i,0,100)
}
WeatherData[8206,]
```

We were successful in getting in to shape.We implemtened the same thing for the other features:

 Dew Point
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$Dew_PointF < -20 | WeatherData$Dew_PointF > 80 | is.na(WeatherData$Dew_PointF))
for (i in index){
  WeatherData$Dew_PointF[i] = remove_out(WeatherData$Dew_PointF,i,-20,80)
}
```

 Humidity
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$Humidity < 10 | WeatherData$Humidity > 100 | is.na(WeatherData$Humidity))
for (i in index){
  WeatherData$Humidity[i] = remove_out(WeatherData$Humidity,i,10,100)
}
```

 Wind_SpeedMPH
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$Wind_SpeedMPH < 0 | WeatherData$Wind_SpeedMPH > 50 | is.na(WeatherData$Wind_SpeedMPH))
for (i in index){
  WeatherData$Wind_SpeedMPH[i] = remove_out(WeatherData$Wind_SpeedMPH,i,0,50)
}
```

Sea_Level_Pressure
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$Sea_Level_PressureIn < 28 | WeatherData$Sea_Level_PressureIn > 32 | is.na(WeatherData$Sea_Level_PressureIn))
for (i in index){
  WeatherData$Sea_Level_PressureIn[i] = remove_out(WeatherData$Sea_Level_PressureIn,i,28,32)
}
```

 VisibilityMPH
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$VisibilityMPH < 0 | WeatherData$VisibilityMPH > 10 | is.na(WeatherData$VisibilityMPH))
for (i in index){
  WeatherData$VisibilityMPH[i] = remove_out(WeatherData$VisibilityMPH,i,0,10)
}
```

 WindDirDegree
```{r,message=FALSE,warning=FALSE}
index <- which(WeatherData$WindDirDegrees < 0 | WeatherData$WindDirDegrees > 360 | is.na(WeatherData$WindDirDegrees))
for (i in index){
  WeatherData$WindDirDegrees[i] = remove_out(WeatherData$WindDirDegrees,i,0,360)
}
```

 The data was clean and consistent. We aggregated the dataset as done in part 1, so that we get records for each hour and we can take average values for numeric values and frequency count for character values.
We followed the below steps:
 1) Remove non essential features like Time, Gust_speedMH,P,E
 2) Group the data by Date and hour 
 3) summarise base on mean and frequency count


```{r,message=FALSE,warning=FALSE}
WeatherData.Agg <- WeatherData %>% 
  select(-c(Time,Gust_SpeedMPH,
            PrecipitationIn,Events)) %>%
  group_by(date,hour) %>% 
  summarise(TemperatureF = mean(TemperatureF),
                                    Dew_PointF = mean(Dew_PointF),
                                    Humidity = mean(Humidity),
                                    Sea_Level_PressureIn = mean(Sea_Level_PressureIn),
                                    VisibilityMPH = mean (VisibilityMPH),
                                    Wind_SpeedMPH = mean(Wind_SpeedMPH),
                                    WindDirDegrees = mean(WindDirDegrees),
            Conditions = names(table(Conditions))[which.max(table(Conditions))],
            Wind_Direction = names(table(Wind_Direction))[which.max(table(Wind_Direction))])

head(WeatherData.Agg)
```

Once We had both the dataset wit  in the desired format
We merged the data with part 1 of the energy usage data by Date and hour

###4.3 Final Output Data
```{r,message=FALSE,warning=FALSE}
mergeData <- merge(aggData.long,WeatherData.Agg,by.x = c("Date","hour"),by.y = c("date","hour"))
head(mergeData)

```

Arranging the data by Date and hour
```{r,message=FALSE,warning=FALSE}
mergeData<- arrange(mergeData,Date,hour)
head(mergeData)
```

We write this output to csv file
```{r,message=FALSE,warning=FALSE}
write.csv(mergeData,"MergedData.csv")
```

Now we have the clean data to start with our model